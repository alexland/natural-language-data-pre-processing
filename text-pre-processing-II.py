
# coding: utf-8

# In[1]:

# imports, configs, etc.

import os
import sys
import re
from copy import deepcopy
import collections as CL
import itertools as IT
import numpy as NP
from scipy import linalg as LA
from sklearn import svm as SVM
from IPython.display import display


# In[2]:

from sympy.interactive import printing
import sympy as SYM
from sympy import Matrix as MAT
from sympy.mpmath import *
printing.init_printing()


# In[3]:

from IPython.external import mathjax; mathjax.install_mathjax()


# In[4]:

from matplotlib import pyplot as PLT


# In[5]:

# imports, configs, etc.

import os
import sys
import re
import csv as CSV
from copy import deepcopy
import collections as CL
import itertools as IT

import warnings
warnings.filterwarnings("ignore")

import numpy as NP
from scipy import linalg as LA
from IPython.display import display
from sympy.interactive import printing
import sympy as SYM
from sympy import Matrix as MAT
from sympy.mpmath import *
printing.init_printing()

from IPython.external import mathjax; mathjax.install_mathjax()

get_ipython().magic('matplotlib inline')
from matplotlib import pyplot as PLT
from matplotlib import cm as CM
from mpl_toolkits import axes_grid1 as AG
from mpl_toolkits.mplot3d import Axes3D as AX

NP.set_printoptions(precision=3, suppress=True)
PLT.rcParams['figure.figsize'] = (11.0, 7.5)

my_font_config = {'family' : 'sans-serif',
        'color'  : '#2A52BE',
        'weight' : 'normal',
        'size'   : 14,
        }

from nltk.corpus import stopwords

DATA_DIR = "~/data"
DATA_DIR = os.path.expanduser(DATA_DIR)
PROJ_DIR = os.path.join(DATA_DIR, "mobile-apps")


# In[6]:

labels_file = os.path.join(PROJ_DIR, "class_labels.txt")
data_file = os.path.join(PROJ_DIR, "data.txt")


# In[7]:

SW = stopwords.words('english')

import re
ptn_nwc = "[!#$%&'*+/=?`{|}~^.-]"
ptn_nwc_obj = re.compile(ptn_nwc, re.MULTILINE)

# open the two files
# read them in
# normalize: lower case the text 
# remove end-of-line whitespace
# remove punctuation
# tokenize the lines

with open(data_file, mode='r', encoding='utf-8') as fh:
    d = ( ptn_nwc_obj.sub('', line.strip().lower()).split() 
         for line in fh.readlines() )

with open(labels_file, mode='r', encoding='utf-8') as fh:
    l = [ int(line.strip()) for line in fh.readlines() ]    
    
# remove 'stop words' (using the NLTK set) &
# remove words comprised of three letters or fewer
d = (filter(lambda v: (v not in SW) & (len(v) > 4), line) for line in d)
d = deepcopy([list(line) for line in d])

# remove frequent terms common to all mobile apps:
# (generated by scraping the app summaries 
# from AppData's 1000 most popular mobiles apps)
DOMAIN_STOP_WORDS = ['android', 'free', 'iphone', 'twitter', 'download', 
                      'feature', 'features', 'applications', 'application', 
                      'user', 'users', 'version', 'versions', 'facebook', 
                      'phone', 'available', 'using', 'information', 'provide',
                      'include', 'every', 'device', 'mobile', 'friend',
                      'different', 'please', 'simple', 'email', 'share', 'follow',
                      'great', 'screen', 'provide', 'acces', 'first', 'sound', 'video',]
                         
d = (filter(lambda v: (v not in DOMAIN_STOP_WORDS), line) for line in d)

# normalize: simple word stemming
def stem(word):
    if word.endswith('s'):
        return word[:-1]
    else:
        return word

d = (list(map(stem, line)) for line in d)

d1 = deepcopy([list(line) for line in d])

lx = NP.array([len(line) for line in d1])

# ~ 75 lines have 10 words or fewer
idx = lx > 10
sum(-idx)

# so (temporarily) filter lines having 10 words or fewer &
# filter their corresponding class labels

idx = idx.tolist()
d = IT.compress(d1, idx)
l = IT.compress(l, idx)


# In[8]:

# partition the data & class labels into class I and class 0

d = deepcopy([list(line) for line in d])
l = deepcopy([line for line in l])

assert len(d) == len(l)

# shuffle both containers
idx = NP.random.permutation(NP.arange(len(d)))
d, l = NP.array(d), NP.array(l, dtype='int8')
d, l = d[idx], l[idx]

L = NP.array(l)
    
idx1 = l==1
idx0 = l==0
d1, l1 = d[idx1], l[idx1]
d0, l0 = d[idx0], l[idx0] 

assert d1.size == l1.size
assert d0.size == l0.size


# In[9]:

# any differences in the size of the raw data instances by class?

ld1 = NP.array([ len(line) for line in d1 ])
ld0 = NP.array([ len(line) for line in d0 ])

print("mean word length of class 1 instances: {0:.2f}".format(ld1.mean()))
print("mean word length of class 0 instances: {0:.2f}".format(ld0.mean()))


# In[10]:

# look at the data by class

words_1 = [ word for line in d1 for word in line ]
words_0 = [ word for line in d0 for word in line ]
words_01 = [ word for line in d for word in line ]

import collections as CL

def term_counter(word_bag):
    """
    returns: dict whose keys are terms and values are absolute counts
        for that term
    pass in: python list of terms
    """
    term_counter = CL.defaultdict(int)
    for term in word_bag:
        term_counter[term] += 1
    return term_counter

term_count_1 = term_counter(words_1)
term_count_0 = term_counter(words_0)
term_count_all = term_counter(words_01)


# In[11]:

term_count_1_sorted = sorted(zip(term_count_1.values(), term_count_1.keys()), reverse=True)
term_count_0_sorted = sorted(zip(term_count_0.values(), term_count_0.keys()), reverse=True)


# In[12]:

v1 = [t[1] for t in term_count_1_sorted[:100]]
v0 = [t[1] for t in term_count_0_sorted[:100]]
v1.extend(v0)


# In[13]:

def build_feature_vector(data, feature_vector):
    """
    returns: a structured 2D data array comprised of in which
        each column encodes one discrete feature; each row
        represents one data instance
    pass in: 
        (i) the data: a nested list in which each list is one data instance,
            or 'bag of words';
        (ii) a template feature vector: a list of terms, comprising a subset
            of the population whose frequency will be counted to to supply
            the values comprising each feature vector 
    this fn transforms a sequence of word bags (each bag is a python list)
        into a structured 1D NumPy array of features
    """
    import numpy as NP
    import collections as CL
    from copy import deepcopy
    fv = set(feature_vector)
    # maps each  most-frequent term to an offset in feature vector
    term_vector_lut = { t:i for i, t in enumerate(fv) }
    # remove all words from each line not in the feature_vector
    d = (filter(lambda q: q in fv, line) for line in data)
    d = deepcopy([list(line) for line in d])
    # initialize the empty 2D NumPy array returned 
    m, n = len(d), len(term_vector_lut)
    D = NP.zeros((m, n))
    dx = CL.defaultdict(int)
    c = 0
    for line in d:
        new_row = NP.zeros(len(fv))
        for w in line:
            idx = term_vector_lut[w]
            new_row[idx] += 1
        D[c,:] = new_row
        c += 1
    return D


# In[14]:

v1 = [t[1] for t in term_count_1_sorted[:50]]
v0 = [t[1] for t in term_count_0_sorted[:50]]

v1.extend(v0)


# In[15]:

D = build_feature_vector(d, v1)


# In[16]:

import warnings
warnings.filterwarnings('ignore', r"object.__format__ with a non-empty format string is deprecated")
                        
a1 = [ (w, c) for c, w in term_count_1_sorted[:50] ]
a0 = [ (w, c) for c, w in term_count_0_sorted[:50] ]
a10 = zip(a1, a0)
H1 = '{0} most frequent terms by class'.format(len(a1))
H1 = "50 most frequent terms in each class"
h2a = 'class I'
h2b = 'class 0'
ula, ulb = 15 * '_', 15 * '_'
print("{0:^50}\n".format(H1))
print("{0:^20}\t{1:^30}".format(h2a, h2b))
print("{0:30}\t{1:35}".format(ula, ulb))
#for i, o in a10:
# print("{0:25}\t{1}".format(i, o))
for i, o in a10:
    print("{0:25}\t{1:35}".format(i, o))


# In[17]:

# shuffle the data
L = L.reshape(-1, 1)
DL = NP.hstack((D, L))
idx = NP.random.permutation(NP.arange(D.shape[0]))
DL = DL[idx,]

D, L = NP.hsplit(DL, [-1])


##### optimization I: add features comprised of frequently occurring _pairs_ of words

# In[20]:

# get pairwise frequencies for most common terms

t1 = [ t for cn, t in term_count_1_sorted[:25] ]
t0 = [ t for cn, t in term_count_0_sorted[:25] ]

fv = set(v1)

tv_lut = { t:i for i, t in enumerate(fv) }

LuT = tv_lut
LuT_r = { i:t for i, t in enumerate(fv) }

# indices for 50 most frequent terms in class I instances
idx_ft1 = [ LuT[term] for term in t1 ]
idx_ft0 = [ LuT[term] for term in t0 ]

D, L = NP.hsplit(DL, [-1])
L = L.squeeze()

D = NP.where(D>0, 1, 0)
D = NP.array(D, dtype=bool)

idx1 = L==1
idx0 = L==0

# class I instances
D1 = D[idx1,]

#class 0 instances
D0 = D[idx0,]

# now select just those columns that represent a top 20 term
DT1 = D1.T[idx_ft1,]
DT0 = D0.T[idx_ft0,]

# remap offsets in new arrays (DT1, DT2) to the offsets for the complete dataset
lut_ft1, lut_ft0 = dict(enumerate(idx_ft1)), dict(enumerate(idx_ft0))


# In[21]:

import itertools as IT

def cofreq_score(v1, v2):
    """
    returns: scalar that represents co-occurrence frequency
    pass in: 2 x 1D NumPy arrays of dtype 'bool'
    """
    return NP.sum(v1 & v2)


def prep(t, D=DT1):
    """
    returns: v1, v2 to pass to cofreq_score
    pass in: 
        (i) a tuple of offsets;
        (ii) a transposed dataset as a 2D NumPy array
    """
    idx_v1, idx_v2 = t
    return D[idx_v1], D[idx_v2]


# In[22]:

term_pair_offsets = IT.combinations(range(DT1.shape[0]), 2)

C = [ ( cofreq_score(*prep(t)), t )  for i, t in enumerate(term_pair_offsets) ]

C = sorted(list(C), reverse=True)
for score, t in C:
    print("{0}\t{1}".format(score, t))


##### optimization II: improve classifier accuracy by applying a weight vector to each feature vector

# In[23]:

term_count_1 = term_counter(words_1)
term_count_0 = term_counter(words_0)
term_count_all = term_counter(words_01)


# In[24]:

term_top_20_class1 = a1[:20]
term_top_20_class0 = a0[:20]

# constructing the weight vector:

for t in term_top_20_class1:
    print("term: {0}\t weight: {1:.2f}".format(t[0], t[1]/term_count_0[t[0]]))


# In[25]:

# the other portion of the weight vector:

for t in term_top_20_class0:
    print("term: {0}\t weight: {1:.2f}".format(t[0], t[1]/(term_count_1[t[0]]+1)))


# In[26]:

words_1 = [ word for line in d1 for word in line ]
words_0 = [ word for line in d0 for word in line ]
words_01 = [ word for line in d for word in line ]

a1 = [ (w, c) for c, w in term_count_1_sorted[:20] ]
a0 = [ (w, c) for c, w in term_count_0_sorted[:20] ]

a10 = zip(a1, a0)
H1 = '{0} most frequent terms by class'.format(len(a1))
H1 = "50 most frequent terms in each class"
h2a = 'class I'
h2b = 'class 0'
ula, ulb = 15 * '_', 15 * '_'
print("{0:^50}\n".format(H1))
print("{0:^20}\t{1:^30}".format(h2a, h2b))
print("{0:30}\t{1:35}".format(ula, ulb))
#for i, o in a10:
# print("{0:25}\t{1}".format(i, o))
for i, o in a10:
    print("{0:25}\t{1:35}".format(i, o))


# In[32]:

def partition_data(data, class_labels, train_test_ratio=.9):
    """
    returns: data & class labels, split into training and test groups,
        as 2 x 2-tuples; 
        these 2 containers are suitable to pass to scikit-learn classifier
        objects
        to call their 'fit' method, pass in *tr; 
        for 'predict', pass in te[0];
        for 'score' pass in *te
    pass in: 
        data, 2D NumPy array
        class labels, 1D NumPy array
        train:test ratio: 0 < f < 1, default is 0.9
    bind the result returned from call to this fn to two variables, 
        like so: tr, te = partition_data()
        each variable represents a tuple comprised of data + labels
    """
    # create a vector that holds the row indices
    NP.random.seed(0)
    idx = NP.random.permutation(data.shape[0])
    # now order both data and class labels arrays by idx
    D = data[idx,]
    L = class_labels[idx]
    # allocate the data to test & train partitions according to
    # the train_test_ratio passed in
    q = int(NP.ceil(train_test_ratio * D.shape[0]))
    D_tr = D[:q,:]
    D_te = D[q:,:]
    L_tr = L[:q]
    L_te = L[q:]
    assert D_tr.shape[0] + D_te.shape[0] == D.shape[0]
    assert L_tr.shape[0] + L_te.shape[0] == L.shape[0]
    # 1D array required by scikit-learn
    L_tr, L_te = NP.squeeze(L_tr), NP.squeeze(L_te)
    return (D_tr, L_tr), (D_te, L_te)


# In[33]:

# partition the data into training & test sets
# tr, te = partition_data(D, L)


# In[34]:

# from sklearn import datasets
# iris = datasets.load_iris()
# data = iris.data
# labels = iris.target


# In[35]:

# import the scikit-learn library that includes the three main Naive Bayes classifier factories
from sklearn import naive_bayes as NB


# In[36]:

tr, te = partition_data(D, L)


# In[37]:

# instantiate a Gaussian Naive Bayesian classifier
# gnb = NB.GaussianNB()
mnb = NB.MultinomialNB(alpha=0.01, fit_prior=False)


# In[38]:

mnb.fit(*tr)


# In[39]:

mnb_pred = mnb.predict(te[0])


# In[40]:

print("{0:.2f}".format(mnb.score(*te)))


# In[ ]:



